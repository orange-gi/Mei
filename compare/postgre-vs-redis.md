# PostgreSQL æ›¿ä»£ Redis å®æˆ˜æŒ‡å—

## æ–‡ç« æ¦‚è¿°

æœ¬æ–‡è®°å½•äº†ä¸€ä½å¼€å‘è€…åœ¨å®é™…é¡¹ç›®ä¸­å°† Redis å®Œå…¨æ›¿æ¢ä¸º PostgreSQL çš„å®Œæ•´ç»å†ã€‚ä½œè€…åŸæœ¬é‡‡ç”¨å…¸å‹çš„ Web åº”ç”¨æŠ€æœ¯æ ˆï¼šä½¿ç”¨ PostgreSQL å­˜å‚¨æŒä¹…åŒ–æ•°æ®ï¼Œä½¿ç”¨ Redis å¤„ç†ç¼“å­˜ã€å‘å¸ƒè®¢é˜…å’Œåå°ä»»åŠ¡ã€‚åœ¨å‘ç° PostgreSQL èƒ½å¤Ÿå®ç° Redis çš„æ‰€æœ‰åŠŸèƒ½åï¼Œä½œè€…è¿›è¡Œäº†å…¨é¢è¿ç§»ï¼Œå¹¶è¯¦ç»†è®°å½•äº†è¿ç§»è¿‡ç¨‹ä¸­çš„æŠ€æœ¯å®ç°ã€æ€§èƒ½å¯¹æ¯”ã€æˆæœ¬åˆ†æå’Œæœ€ç»ˆç»“è®ºã€‚

æ–‡ç« æä¾›äº†ä¸°å¯Œçš„ä»£ç ç¤ºä¾‹ã€æ€§èƒ½åŸºå‡†æµ‹è¯•æ•°æ®ã€åˆ†é˜¶æ®µè¿ç§»ç­–ç•¥ä»¥åŠå†³ç­–çŸ©é˜µï¼Œæ—¨åœ¨å¸®åŠ©å…¶ä»–å¼€å‘è€…åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­é‡‡ç”¨ç±»ä¼¼çš„æ–¹æ¡ˆã€‚

---

## ä¸€ã€ä¸ºä»€ä¹ˆ PostgreSQL å¯ä»¥æ›¿ä»£ Redis

### 1.1 æˆæœ¬æ•ˆç›Šåˆ†æ

ä»ç»æµè§’åº¦æ¥çœ‹ï¼Œå•ç‹¬ç»´æŠ¤ Redis åŸºç¡€è®¾æ–½çš„æˆæœ¬ç›¸å½“å¯è§‚ã€‚ä½œè€…çš„ Redis éƒ¨ç½²åœ¨ AWS ElastiCache ä¸Šï¼Œ2GB å†…å­˜çš„é…ç½®æ¯æœˆèŠ±è´¹çº¦ 45 ç¾å…ƒï¼Œè€Œå¦‚æœéœ€è¦æ‰©å±•åˆ° 5GBï¼Œæˆæœ¬å°†é£™å‡è‡³æ¯æœˆ 110 ç¾å…ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä½œè€…å·²ç»ä¸º PostgreSQL æ”¯ä»˜äº†æ¯æœˆ 50 ç¾å…ƒçš„ RDS è´¹ç”¨ï¼ˆåŒ…å« 20GB å­˜å‚¨ï¼‰ï¼Œé¢å¤–çš„ 5GB æ•°æ®å­˜å‚¨ä»…éœ€é¢å¤–æ”¯ä»˜ 0.50 ç¾å…ƒã€‚é€šè¿‡æ¶ˆé™¤ç‹¬ç«‹çš„ Redis åŸºç¡€è®¾æ–½ï¼Œä½œè€…æ¯æœˆå¯èŠ‚çœçº¦ 100 ç¾å…ƒçš„è´¹ç”¨ã€‚

è¿™ä¸€æˆæœ¬å·®å¼‚åœ¨è§„æ¨¡æ‰©å¤§æ—¶ä¼šæ›´åŠ æ˜¾è‘—ã€‚Redis ä½œä¸ºå†…å­˜æ•°æ®åº“ï¼Œå…¶å­˜å‚¨æˆæœ¬ä¸æ•°æ®é‡æˆæ­£æ¯”ï¼›è€Œ PostgreSQL ä½œä¸ºç£ç›˜å­˜å‚¨çš„æ•°æ®åº“ï¼Œå¢é‡å­˜å‚¨æˆæœ¬æä½ã€‚å¯¹äºæ•°æ®é‡æŒç»­å¢é•¿çš„åº”ç”¨æ¥è¯´ï¼Œè¿™ç§æˆæœ¬ä¼˜åŠ¿ä¼šè¶Šæ¥è¶Šæ˜æ˜¾ã€‚

### 1.2 è¿ç»´å¤æ‚åº¦é™ä½

ç®¡ç†ä¸¤ä¸ªç‹¬ç«‹çš„æ•°æ®åº“ç³»ç»Ÿä¼šæ˜¾è‘—å¢åŠ è¿ç»´è´Ÿæ‹…ã€‚ä½¿ç”¨ Redis æ—¶ï¼Œä½œè€…é¢ä¸´è¯¸å¤šä¸ç¡®å®šæ€§ï¼šå¤‡ä»½ç­–ç•¥åº”è¯¥å¦‚ä½•é€‰æ‹©ï¼ˆRDB å¿«ç…§ã€AOF æ—¥å¿—ï¼Œè¿˜æ˜¯ä¸¤è€…ç»“åˆï¼‰ï¼Ÿç›‘æ§é…ç½®åº”è¯¥å¦‚ä½•è®¾ç½®ï¼Ÿæ•…éšœè½¬ç§»åº”è¯¥é‡‡ç”¨ Redis Sentinel è¿˜æ˜¯ Cluster æ–¹æ¡ˆï¼Ÿè¿™äº›é—®é¢˜éƒ½éœ€è¦ä¸“ä¸šçŸ¥è¯†æ¥è§£å†³ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼ŒPostgreSQL çš„å¤‡ä»½ã€ç›‘æ§å’Œæ•…éšœè½¬ç§»æµç¨‹å·²ç»éå¸¸æˆç†Ÿï¼Œæœ‰å®Œå–„çš„æ–‡æ¡£å’Œå·¥å…·æ”¯æŒã€‚é€šè¿‡å°†æ‰€æœ‰åŠŸèƒ½ç»Ÿä¸€åˆ° PostgreSQLï¼Œä½œè€…å°†è¿ç»´å¤æ‚åº¦é™ä½äº†çº¦ 50%ï¼Œåªéœ€è¦å…³æ³¨ä¸€ä¸ªæ•°æ®åº“ç³»ç»Ÿçš„å¥åº·çŠ¶å†µå’Œæ€§èƒ½ä¼˜åŒ–ã€‚

### 1.3 æ•°æ®ä¸€è‡´æ€§ä¿è¯

åœ¨ä½¿ç”¨åˆ†ç¦»çš„ç¼“å­˜å’Œæ•°æ®åº“å±‚æ—¶ï¼Œä¿æŒæ•°æ®ä¸€è‡´æ€§æ˜¯ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ã€‚å…¸å‹çš„æ“ä½œæµç¨‹åŒ…æ‹¬ï¼šå…ˆæ›´æ–°æ•°æ®åº“ï¼Œç„¶åä½¿ç¼“å­˜å¤±æ•ˆã€‚ä½†è¿™ä¸ªè¿‡ç¨‹ä¸­å­˜åœ¨å¤šä¸ªå¯èƒ½çš„å¤±è´¥ç‚¹ï¼šRedis å¯èƒ½ä¸å¯ç”¨ï¼Œä½¿ç¼“å­˜å¤±æ•ˆçš„è°ƒç”¨å¯èƒ½å¤±è´¥ï¼Œç½‘ç»œé—®é¢˜å¯èƒ½ä¸­æ–­æ“ä½œã€‚ä¸€æ—¦å‘ç”Ÿä»»ä½•é—®é¢˜ï¼Œç¼“å­˜å’Œæ•°æ®åº“å°±ä¼šé™·å…¥ä¸ä¸€è‡´çŠ¶æ€ã€‚

PostgreSQL é€šè¿‡åŸå­äº‹åŠ¡å½»åº•è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚æ•°æ®åº“æ“ä½œã€ç¼“å­˜æ›´æ–°å’Œé€šçŸ¥å‘å¸ƒå¯ä»¥åœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­æ‰§è¡Œï¼Œè¦ä¹ˆå…¨éƒ¨æˆåŠŸï¼Œè¦ä¹ˆå…¨éƒ¨å›æ»šã€‚è¿™ç§äº‹åŠ¡æ€§ä¿è¯æ¶ˆé™¤äº†åˆ†å¸ƒå¼ç³»ç»Ÿä¸­æ•°æ®ä¸ä¸€è‡´çš„æ ¹æœ¬åŸå› ï¼Œå¤§å¤§ç®€åŒ–äº†åº”ç”¨é€»è¾‘å¹¶æé«˜äº†ç³»ç»Ÿå¯é æ€§ã€‚

---

## äºŒã€PostgreSQL æ›¿ä»£ Redis çš„æ ¸å¿ƒæŠ€æœ¯æ–¹æ¡ˆ

### 2.1 ç¼“å­˜åŠŸèƒ½ï¼šUNLOGGED è¡¨

PostgreSQL çš„ UNLOGGED è¡¨æ˜¯å®ç°ç¼“å­˜åŠŸèƒ½çš„ç†æƒ³é€‰æ‹©ã€‚UNLOGGED è¡¨åœ¨å†™å…¥æ—¶ä¼šè·³è¿‡é¢„å†™æ—¥å¿—ï¼ˆWALï¼‰ï¼Œè¿™ä½¿å¾—å†™å…¥æ“ä½œæ›´å¿«ï¼ŒåŒæ—¶åœ¨æ­£å¸¸æ“ä½œä¸­ä»ç„¶ä¿æŒæ•°æ®è€ä¹…æ€§ã€‚å”¯ä¸€çš„ç‰¹ç‚¹æ˜¯å®ƒä»¬ä¸ä¼šåœ¨æ•°æ®åº“å´©æºƒåæ¢å¤ï¼Œä½†å¯¹äºç¼“å­˜æ•°æ®æ¥è¯´ï¼Œè¿™å®Œå…¨æ˜¯å¯ä»¥æ¥å—çš„ï¼Œå› ä¸ºç¼“å­˜æ•°æ®å¯ä»¥éšæ—¶é‡æ–°ç”Ÿæˆã€‚

ä½œè€…åˆ›å»ºäº†ä¸€ä¸ªä¸“é—¨çš„ç¼“å­˜è¡¨ç»“æ„ï¼ŒåŒ…å« keyï¼ˆæ–‡æœ¬ä¸»é”®ï¼‰ã€valueï¼ˆJSONB ç±»å‹å­˜å‚¨å®é™…æ•°æ®ï¼‰å’Œ expires_atï¼ˆè¿‡æœŸæ—¶é—´æˆ³ï¼‰ä¸‰ä¸ªå­—æ®µã€‚é€šè¿‡åœ¨æŸ¥è¯¢ä¸­æ·»åŠ  `expires_at > NOW()` æ¡ä»¶æ¥å®ç° TTL åŠŸèƒ½ã€‚æ€§èƒ½æµ‹è¯•æ˜¾ç¤ºï¼ŒRedis çš„ SET æ“ä½œå»¶è¿Ÿä¸º 0.05 æ¯«ç§’ï¼Œè€Œ PostgreSQL çš„ UNLOGGED INSERT å»¶è¿Ÿä¸º 0.08 æ¯«ç§’ï¼Œå¯¹äºç¼“å­˜åœºæ™¯æ¥è¯´è¿™ä¸ªå·®å¼‚å®Œå…¨å¯ä»¥æ¥å—ã€‚

```sql
CREATE UNLOGGED TABLE cache (
  key TEXT PRIMARY KEY,
  value JSONB NOT NULL,
  expires_at TIMESTAMPTZ NOT NULL
);

CREATE INDEX idx_cache_expires ON cache(expires_at);
```

### 2.2 å‘å¸ƒè®¢é˜…ï¼šLISTEN/NOTIFY

PostgreSQL å†…ç½®çš„å‘å¸ƒè®¢é˜…åŠŸèƒ½æ˜¯è®¸å¤šå¼€å‘è€…ä¸äº†è§£çš„éšè—ç‰¹æ€§ã€‚é€šè¿‡ LISTEN å’Œ NOTIFY å‘½ä»¤ï¼Œå¼€å‘è€…å¯ä»¥è½»æ¾å®ç°è¿›ç¨‹é—´é€šä¿¡ã€‚å‘å¸ƒè€…ä½¿ç”¨ NOTIFY å‘½ä»¤å‘é€æ¶ˆæ¯ï¼Œè®¢é˜…è€…é€šè¿‡ LISTEN å‘½ä»¤ç›‘å¬ç‰¹å®šé€šé“å¹¶æ¥æ”¶å¼‚æ­¥é€šçŸ¥ã€‚

æ€§èƒ½å¯¹æ¯”æ˜¾ç¤ºï¼ŒRedis çš„å‘å¸ƒè®¢é˜…å»¶è¿Ÿä¸º 1-2 æ¯«ç§’ï¼Œè€Œ PostgreSQL ä¸º 2-5 æ¯«ç§’ã€‚è™½ç„¶ PostgreSQL ç¨æ…¢ï¼Œä½†å…¶ç‹¬ç‰¹ä¼˜åŠ¿åœ¨äºå¯ä»¥åœ¨äº‹åŠ¡ä¸­å‘é€é€šçŸ¥ï¼Œå¹¶ä¸”å¯ä»¥ä¸æŸ¥è¯¢ç»“åˆå®ç°æ›´å¤æ‚çš„æ¨¡å¼ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡æ•°æ®åº“è§¦å‘å™¨ï¼Œå¯ä»¥åœ¨æ’å…¥æ–°æ•°æ®æ—¶è‡ªåŠ¨è§¦å‘é€šçŸ¥ï¼Œå®ç°çœŸæ­£çš„æ•°æ®é©±åŠ¨é€šçŸ¥ã€‚

```sql
-- å‘å¸ƒé€šçŸ¥
NOTIFY notifications, '{"userId": 123, "msg": "Hello"}';

-- ç›‘å¬é€šçŸ¥
LISTEN notifications;
```

### 2.3 ä»»åŠ¡é˜Ÿåˆ—ï¼šSKIP LOCKED

PostgreSQL çš„ SKIP LOCKED ç‰¹æ€§ä½¿å…¶æˆä¸ºä¸€ä¸ªå¼ºå¤§çš„æ— é”ä»»åŠ¡é˜Ÿåˆ—ã€‚å½“å¤šä¸ªå·¥ä½œè¿›ç¨‹åŒæ—¶å°è¯•è·å–ä»»åŠ¡æ—¶ï¼ŒSKIP LOCKED ç¡®ä¿æ¯ä¸ªä»»åŠ¡åªåˆ†é…ç»™ä¸€ä¸ªå·¥ä½œè¿›ç¨‹ï¼Œè€Œå…¶ä»–å·¥ä½œè¿›ç¨‹ä¼šè·³è¿‡å·²è¢«é”å®šçš„è¡Œç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡ã€‚è¿™ç§æ¨¡å¼æä¾›äº†å¹¶å‘å¤„ç†èƒ½åŠ›ã€è‡ªåŠ¨é˜²æ­¢é‡å¤æ‰§è¡Œã€ä»¥åŠåœ¨ worker å´©æºƒæ—¶ä»»åŠ¡è‡ªåŠ¨é‡æ–°å¯ç”¨çš„ç‰¹æ€§ã€‚

æ€§èƒ½æµ‹è¯•è¡¨æ˜ï¼ŒRedis çš„ BRPOP æ“ä½œå»¶è¿Ÿä¸º 0.1 æ¯«ç§’ï¼ŒPostgreSQL çš„ SKIP LOCKED æŸ¥è¯¢å»¶è¿Ÿä¸º 0.3 æ¯«ç§’ï¼Œå¯¹äºå¤§å¤šæ•°ç”Ÿäº§å·¥ä½œè´Ÿè½½æ¥è¯´ï¼Œè¿™ä¸ªå·®å¼‚å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚

```sql
WITH next_job AS (
  SELECT id FROM jobs
  WHERE queue = $1
    AND attempts < max_attempts
    AND scheduled_at <= NOW()
  ORDER BY scheduled_at
  LIMIT 1
  FOR UPDATE SKIP LOCKED
)
UPDATE jobs
SET attempts = attempts + 1
FROM next_job
WHERE jobs.id = next_job.id
RETURNING *;
```

### 2.4 é€Ÿç‡é™åˆ¶

PostgreSQL æä¾›äº†å¤šç§å®ç°é€Ÿç‡é™åˆ¶çš„æ–¹æ³•ã€‚ç®€å•çš„é€Ÿç‡é™åˆ¶ä½¿ç”¨è®¡æ•°å™¨è¡¨åŠ æ—¶é—´æˆ³ï¼Œè€Œæ›´å¤æ‚çš„å®ç°åˆ™è®°å½•æ¯ä¸ªè¯·æ±‚ä»¥æ”¯æŒæ—¶é—´çª—å£æŸ¥è¯¢ã€‚PostgreSQL æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºå½“é€Ÿç‡é™åˆ¶é€»è¾‘å¤æ‚æˆ–éœ€è¦ä¸å…¶ä»–ä¸šåŠ¡é€»è¾‘åœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­æ‰§è¡Œæ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚

### 2.5 ä¼šè¯å­˜å‚¨ï¼šJSONB

ä½¿ç”¨ PostgreSQL çš„ JSONB åˆ—å­˜å‚¨ä¼šè¯æ•°æ®ä¸ä»…æä¾›äº† Redis çš„æ‰€æœ‰åŠŸèƒ½ï¼Œè¿˜èƒ½è¿›è¡Œå†…éƒ¨æŸ¥è¯¢ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æŸ¥æ‰¾ç‰¹å®šç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯ï¼Œæˆ–æ ¹æ®å­˜å‚¨åœ¨ JSON ä¸­çš„è§’è‰²ç­›é€‰ä¼šè¯ã€‚è¿™ç§æŸ¥è¯¢èƒ½åŠ›åœ¨ä½¿ç”¨ Redis çš„å­—ç¬¦ä¸²å­˜å‚¨æ—¶æ˜¯ä¸å¯èƒ½å®ç°çš„ã€‚

```sql
CREATE TABLE sessions (
  id TEXT PRIMARY KEY,
  data JSONB NOT NULL,
  expires_at TIMESTAMPTZ NOT NULL
);

-- æŸ¥æ‰¾ç‰¹å®šç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯
SELECT * FROM sessions
WHERE data->>'userId' = '123';
```

---

## ä¸‰ã€æ€§èƒ½åŸºå‡†æµ‹è¯•å¯¹æ¯”

### 3.1 ç‹¬ç«‹æ“ä½œæ€§èƒ½

ä½œè€…åœ¨ AWS RDS db.t3.medium å®ä¾‹ï¼ˆ2 vCPUï¼Œ4GB RAMï¼‰ä¸Šä½¿ç”¨åŒ…å« 100 ä¸‡æ¡ç¼“å­˜è®°å½•å’Œ 1 ä¸‡ä¸ªä¼šè¯çš„ç”Ÿäº§æ•°æ®é›†è¿›è¡Œäº†æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒPostgreSQL åœ¨æ‰€æœ‰æ“ä½œä¸Šéƒ½æ¯” Redis æ…¢ï¼Œä½†æ‰€æœ‰æ“ä½œä»ç„¶ä¿æŒåœ¨ 1 æ¯«ç§’ä»¥ä¸‹ã€‚

| æ“ä½œç±»å‹ | Redis | PostgreSQL | å·®å¼‚ |
|---------|-------|------------|------|
| ç¼“å­˜å†™å…¥ | 0.05ms | 0.08ms | æ…¢ 60% |
| ç¼“å­˜è¯»å– | 0.04ms | 0.06ms | æ…¢ 50% |
| å‘å¸ƒè®¢é˜… | 1.2ms | 3.1ms | æ…¢ 158% |
| é˜Ÿåˆ—æ¨é€ | 0.08ms | 0.15ms | æ…¢ 87% |
| é˜Ÿåˆ—å¼¹å‡º | 0.12ms | 0.31ms | æ…¢ 158% |

### 3.2 ç»„åˆæ“ä½œæ€§èƒ½

ç„¶è€Œï¼Œä½œè€…å‘ç°å½“å¤šä¸ªæ“ä½œéœ€è¦ç»„åˆæ‰§è¡Œæ—¶ï¼ŒPostgreSQL ç”±äºæ¶ˆé™¤äº†ç½‘ç»œå¾€è¿”ï¼Œå®é™…æ€§èƒ½å¯èƒ½æ›´ä¼˜ã€‚åœ¨ä¸€ä¸ªå…¸å‹çš„åœºæ™¯ä¸­â€”â€”æ’å…¥æ•°æ®ã€ä½¿ç¼“å­˜å¤±æ•ˆã€é€šçŸ¥è®¢é˜…è€…â€”â€”ä½¿ç”¨ Redis éœ€è¦çº¦ 4 æ¯«ç§’ï¼ˆ2 æ¯«ç§’æ•°æ®åº“æ“ä½œ + 1 æ¯«ç§’ç¼“å­˜åˆ é™¤ç½‘ç»œå¾€è¿” + 1 æ¯«ç§’å‘å¸ƒç½‘ç»œå¾€è¿”ï¼‰ã€‚è€Œ PostgreSQL åœ¨å•ä¸€æ•°æ®åº“è¿æ¥ä¸Šæ‰§è¡Œæ‰€æœ‰æ“ä½œåªéœ€çº¦ 2.2 æ¯«ç§’ï¼Œæ¶ˆé™¤äº†ç½‘ç»œå¼€é”€ã€‚

---

## å››ã€ä½•æ—¶ä¿ç•™ Redis

è™½ç„¶ PostgreSQL å¯ä»¥æ›¿ä»£ Redisï¼Œä½†ä½œè€…å¼ºè°ƒè¿™ç§æ›¿ä»£å¹¶éæ™®éé€‚ç”¨ã€‚ä»¥ä¸‹æƒ…å†µå»ºè®®ä¿ç•™ Redisï¼š

**æç«¯æ€§èƒ½éœ€æ±‚**ï¼šRedis åœ¨å•å®ä¾‹ä¸Šå¯ä»¥å¤„ç† 10 ä¸‡æ¬¡ä»¥ä¸Šçš„æ“ä½œæ¯ç§’ï¼Œè€Œ PostgreSQL é€šå¸¸å¤„ç†èƒ½åŠ›åœ¨ 1 ä¸‡åˆ° 5 ä¸‡æ¬¡æ¯ç§’ã€‚å¯¹äºæ¯ç§’å¤„ç†æ•°ç™¾ä¸‡æ¬¡ç¼“å­˜è¯»å–çš„åº”ç”¨ï¼ŒRedis ä»æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚

**Redis ç‰¹æœ‰æ•°æ®ç»“æ„**ï¼šRedis æä¾›äº†è®¸å¤š PostgreSQL éš¾ä»¥é«˜æ•ˆå®ç°çš„æ•°æ®ç»“æ„ï¼ŒåŒ…æ‹¬ç”¨äºæ’è¡Œæ¦œçš„æœ‰åºé›†åˆã€ç”¨äºå”¯ä¸€è®¡æ•°ä¼°è®¡çš„ HyperLogLogã€ç”¨äºåœ°ç†ä½ç½®æŸ¥è¯¢çš„åœ°ç†ç©ºé—´ç´¢å¼•ï¼Œä»¥åŠç”¨äºé«˜çº§å‘å¸ƒè®¢é˜…çš„ Streamsã€‚

**ç‹¬ç«‹ç¼“å­˜å±‚æ¶æ„**ï¼šåœ¨å¾®æœåŠ¡æ¶æ„ä¸­ï¼Œå¦‚æœéœ€è¦ç‹¬ç«‹ç¼“å­˜å±‚æˆ–å¤šä¸ªæœåŠ¡å…±äº«ç¼“å­˜ï¼ŒRedis çš„ä¸“é—¨è®¾è®¡å…·æœ‰ä¼˜åŠ¿ã€‚

**ä¸“ä¸šè¿ç»´å›¢é˜Ÿ**ï¼šæ‹¥æœ‰ä¸“ä¸šåŸºç¡€è®¾æ–½å›¢é˜Ÿçš„ç»„ç»‡å¯ä»¥ç®¡ç† Redis çš„è¿ç»´å¤æ‚æ€§å¹¶ä»ä¸­è·å–æœ€å¤§æ€§èƒ½ã€‚

---

## äº”ã€åˆ†é˜¶æ®µè¿ç§»ç­–ç•¥

ä½œè€…å»ºè®®é‡‡ç”¨æ¸è¿›å¼è¿ç§»ç­–ç•¥ï¼Œæ•´ä¸ªè¿‡ç¨‹çº¦éœ€å››å‘¨æ—¶é—´ï¼š

**ç¬¬ä¸€é˜¶æ®µï¼ˆç¬¬ 1 å‘¨ï¼‰â€”â€”åŒå†™é˜¶æ®µ**ï¼šåŒæ—¶å‘ Redis å’Œ PostgreSQL å†™å…¥ï¼Œç»§ç»­ä» Redis è¯»å–ã€‚ç›‘æ§å‘½ä¸­ç‡ã€å»¶è¿Ÿï¼Œå»ºç«‹æ€§èƒ½åŸºçº¿ã€‚

**ç¬¬äºŒé˜¶æ®µï¼ˆç¬¬ 2 å‘¨ï¼‰â€”â€”è¯»åˆ‡æ¢é˜¶æ®µ**ï¼šä¼˜å…ˆä» PostgreSQL è¯»å–ï¼Œå¦‚æœæ•°æ®ä¸å­˜åœ¨åˆ™å›é€€åˆ° Redisã€‚ç›‘æ§é”™è¯¯ç‡å’Œæ€§èƒ½å½±å“ã€‚

**ç¬¬ä¸‰é˜¶æ®µï¼ˆç¬¬ 3 å‘¨ï¼‰â€”â€”å†™åˆ‡æ¢é˜¶æ®µ**ï¼šåªå‘ PostgreSQL å†™å…¥ï¼Œç§»é™¤å†™å…¥è·¯å¾„ä¸­çš„ Redisã€‚ç›‘æ§ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

**ç¬¬å››é˜¶æ®µï¼ˆç¬¬ 4 å‘¨ï¼‰â€”â€”ä¸‹çº¿é˜¶æ®µ**ï¼šå…³é—­ Redisï¼Œç›‘æ§ç³»ç»Ÿæ˜¯å¦å‡ºç°é—®é¢˜ã€‚å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œåˆ™è¿ç§»æˆåŠŸã€‚

---

## å…­ã€ä¸‰ä¸ªæœˆåçš„å®é™…æ•ˆæœ

### 6.1 è·å¾—æ”¶ç›Š

- æ¯æœˆèŠ‚çœ 100 ç¾å…ƒï¼ˆæ— éœ€ ElastiCacheï¼‰
- å¤‡ä»½å¤æ‚åº¦é™ä½ 50%
- å‡å°‘ä¸€ä¸ªéœ€è¦ç›‘æ§çš„æœåŠ¡
- éƒ¨ç½²ä¾èµ–å‡å°‘ï¼Œç®€åŒ–éƒ¨ç½²æµç¨‹

### 6.2 ä»˜å‡ºä»£ä»·

- ç¼“å­˜æ“ä½œå¢åŠ çº¦ 0.5 æ¯«ç§’å»¶è¿Ÿ
- æ— æ³•ä½¿ç”¨ Redis çš„ç‰¹æ®Šæ•°æ®ç»“æ„ï¼ˆä½†é¡¹ç›®ä¸éœ€è¦ï¼‰

### 6.3 æœ€ç»ˆç»“è®º

ä½œè€…è¡¨ç¤ºï¼Œå¦‚æœé‡æ–°é€‰æ‹©ï¼Œä»ç„¶ä¼šè¿›è¡Œè¿™æ¬¡è¿ç§»ï¼Œå› ä¸ºå¯¹äºä»–ä»¬çš„ä½¿ç”¨åœºæ™¯æ¥è¯´æ”¶ç›Šè¿œå¤§äºä»£ä»·ã€‚ä½†ä»–ä»¬ä¸ä¼šæ— å·®åˆ«åœ°æ¨èè¿™ç§æ›¿ä»£æ–¹æ¡ˆã€‚

---

## ä¸ƒã€å†³ç­–çŸ©é˜µ

### åº”è¯¥ç”¨ PostgreSQL æ›¿ä»£ Redis çš„æƒ…å†µ

- ä¸»è¦ä½¿ç”¨ Redis è¿›è¡Œç®€å•ç¼“å­˜æˆ–ä¼šè¯å­˜å‚¨
- ç¼“å­˜å‘½ä¸­ç‡ä½äº 95%ï¼ˆå†™å…¥æ“ä½œè¾ƒå¤šï¼‰
- éœ€è¦äº‹åŠ¡æ€§æ•°æ®ä¸€è‡´æ€§ä¿è¯
- å¯ä»¥æ¥å— 0.1-1 æ¯«ç§’çš„æ“ä½œå»¶è¿Ÿå¢åŠ 
- æ˜¯ç¼ºä¹è¿ç»´èµ„æºçš„å°å‹å›¢é˜Ÿ

### åº”è¯¥ä¿ç•™ Redis çš„æƒ…å†µ

- éœ€è¦æ¯ç§’ 10 ä¸‡æ¬¡ä»¥ä¸Šçš„æ“ä½œååé‡
- ä½¿ç”¨ Redis ç‰¹æœ‰æ•°æ®ç»“æ„ï¼ˆæœ‰åºé›†åˆç­‰ï¼‰
- æ‹¥æœ‰ä¸“ä¸šè¿ç»´å›¢é˜Ÿ
- éœ€è¦äºšæ¯«ç§’çº§å»¶è¿Ÿ
- éœ€è¦åœ°ç†å¤åˆ¶èƒ½åŠ›

---

## å…«ã€æ€»ç»“

PostgreSQL ç¡®å®å¯ä»¥åœ¨è®¸å¤šåœºæ™¯ä¸‹æ›¿ä»£ Redisï¼Œä¸ºå¼€å‘è€…æä¾›æ›´ä½çš„è¿ç»´æˆæœ¬ã€ç®€åŒ–çš„åŸºç¡€è®¾æ–½å’Œæ›´å¼ºçš„æ•°æ®ä¸€è‡´æ€§ä¿è¯ã€‚å…³é”®åœ¨äºç†è§£ä¸¤è€…çš„ç‰¹ç‚¹ï¼Œæ ¹æ®å…·ä½“ä¸šåŠ¡éœ€æ±‚åšå‡ºåˆç†é€‰æ‹©ã€‚å¯¹äºä¸­å°å‹åº”ç”¨ã€ç®€å•çš„ç¼“å­˜éœ€æ±‚ã€è¿½æ±‚ç®€åŒ–è¿ç»´çš„å›¢é˜Ÿï¼ŒPostgreSQL æ˜¯ä¸€ä¸ªå€¼å¾—è€ƒè™‘çš„æ–¹æ¡ˆã€‚å¯¹äºè¿½æ±‚æè‡´æ€§èƒ½ã€éœ€è¦ Redis ç‰¹æœ‰æ•°æ®ç»“æ„ã€æˆ–æœ‰ä¸“ä¸šå›¢é˜Ÿæ”¯æ’‘çš„åœºæ™¯ï¼Œä¿ç•™ Redis ä»ç„¶æ˜¯æ˜æ™ºçš„é€‰æ‹©ã€‚

æŠ€æœ¯é€‰å‹æ²¡æœ‰æ”¾ä¹‹å››æµ·è€Œçš†å‡†çš„æœ€ä½³æ–¹æ¡ˆï¼Œåªæœ‰æœ€é€‚åˆç‰¹å®šåœºæ™¯çš„æ–¹æ¡ˆã€‚å¸Œæœ›æœ¬æ–‡çš„åˆ†æèƒ½å¸®åŠ©è¯»è€…åœ¨ PostgreSQL å’Œ Redis ä¹‹é—´åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚


======================================================================================================================================
åŸæ–‡ï¼šhttps://dev.to/polliog/i-replaced-redis-with-postgresql-and-its-faster-4942
I Replaced Redis with PostgreSQL (And It's Faster)
#
postgres
#
redis
#
database
#
performance
I had a typical web app stack:

PostgreSQL for persistent data
Redis for caching, pub/sub, and background jobs
Two databases. Two things to manage. Two points of failure.

Then I realized: PostgreSQL can do everything Redis does.

I ripped out Redis entirely. Here's what happened.

The Setup: What I Was Using Redis For
Before the change, Redis handled three things:

1. Caching (70% of usage)
// Cache API responses
await redis.set(`user:${id}`, JSON.stringify(user), 'EX', 3600);
2. Pub/Sub (20% of usage)
// Real-time notifications
redis.publish('notifications', JSON.stringify({ userId, message }));
3. Background Job Queue (10% of usage)
// Using Bull/BullMQ
queue.add('send-email', { to, subject, body });
The pain points:

Two databases to backup
Redis uses RAM (expensive at scale)
Redis persistence is... complicated
Network hop between Postgres and Redis
Why I Considered Replacing Redis
Reason #1: Cost
My Redis setup:

AWS ElastiCache: $45/month (2GB)
Growing to 5GB would cost $110/month
PostgreSQL:

Already paying for RDS: $50/month (20GB storage)
Adding 5GB of data: $0.50/month
Potential savings: ~$100/month

Reason #2: Operational Complexity
With Redis:

Postgres backup âœ…
Redis backup â“ (RDB? AOF? Both?)
Postgres monitoring âœ…
Redis monitoring â“
Postgres failover âœ…
Redis Sentinel/Cluster â“
Without Redis:

Postgres backup âœ…
Postgres monitoring âœ…
Postgres failover âœ…
One less moving part.

Reason #3: Data Consistency
The classic problem:

// Update database
await db.query('UPDATE users SET name = $1 WHERE id = $2', [name, id]);

// Invalidate cache
await redis.del(`user:${id}`);

// âš ï¸ What if Redis is down?
// âš ï¸ What if this fails?
// Now cache and DB are out of sync
With everything in Postgres: transactions solve this.

PostgreSQL Feature #1: Caching with UNLOGGED Tables
Redis:

await redis.set('session:abc123', JSON.stringify(sessionData), 'EX', 3600);
PostgreSQL:

CREATE UNLOGGED TABLE cache (
  key TEXT PRIMARY KEY,
  value JSONB NOT NULL,
  expires_at TIMESTAMPTZ NOT NULL
);

CREATE INDEX idx_cache_expires ON cache(expires_at);
Insert:

INSERT INTO cache (key, value, expires_at)
VALUES ($1, $2, NOW() + INTERVAL '1 hour')
ON CONFLICT (key) DO UPDATE
  SET value = EXCLUDED.value,
      expires_at = EXCLUDED.expires_at;
Read:

SELECT value FROM cache
WHERE key = $1 AND expires_at > NOW();
Cleanup (run periodically):

DELETE FROM cache WHERE expires_at < NOW();
What is UNLOGGED?
UNLOGGED tables:

Skip the Write-Ahead Log (WAL)
Much faster writes
Don't survive crashes (perfect for cache!)
Performance:

Redis SET: 0.05ms
Postgres UNLOGGED INSERT: 0.08ms
Close enough for caching.

PostgreSQL Feature #2: Pub/Sub with LISTEN/NOTIFY
This is where it gets interesting.

PostgreSQL has native pub/sub that most developers don't know about.

Redis Pub/Sub
// Publisher
redis.publish('notifications', JSON.stringify({ userId: 123, msg: 'Hello' }));

// Subscriber
redis.subscribe('notifications');
redis.on('message', (channel, message) => {
  console.log(message);
});
PostgreSQL Pub/Sub
-- Publisher
NOTIFY notifications, '{"userId": 123, "msg": "Hello"}';
// Subscriber (Node.js with pg)
const client = new Client({ connectionString: process.env.DATABASE_URL });
await client.connect();

await client.query('LISTEN notifications');

client.on('notification', (msg) => {
  const payload = JSON.parse(msg.payload);
  console.log(payload);
});
Performance comparison:

Redis pub/sub latency: 1-2ms
Postgres NOTIFY latency: 2-5ms
Slightly slower, but:

No extra infrastructure
Can use in transactions
Can combine with queries
Real-World Example: Live Tail
In my log management app, I needed real-time log streaming.

With Redis:

// When new log arrives
await db.query('INSERT INTO logs ...');
await redis.publish('logs:new', JSON.stringify(log));

// Frontend listens
redis.subscribe('logs:new');
Problem: Two operations. What if publish fails?

With PostgreSQL:

CREATE FUNCTION notify_new_log() RETURNS TRIGGER AS $$
BEGIN
  PERFORM pg_notify('logs_new', row_to_json(NEW)::text);
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER log_inserted
AFTER INSERT ON logs
FOR EACH ROW EXECUTE FUNCTION notify_new_log();
Now it's atomic. Insert and notify happen together or not at all.

// Frontend (via SSE)
app.get('/logs/stream', async (req, res) => {
  const client = await pool.connect();

  res.writeHead(200, {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
  });

  await client.query('LISTEN logs_new');

  client.on('notification', (msg) => {
    res.write(`data: ${msg.payload}\n\n`);
  });
});
Result: Real-time log streaming with zero Redis.

PostgreSQL Feature #3: Job Queues with SKIP LOCKED
Redis (using Bull/BullMQ):

queue.add('send-email', { to, subject, body });

queue.process('send-email', async (job) => {
  await sendEmail(job.data);
});
PostgreSQL:

CREATE TABLE jobs (
  id BIGSERIAL PRIMARY KEY,
  queue TEXT NOT NULL,
  payload JSONB NOT NULL,
  attempts INT DEFAULT 0,
  max_attempts INT DEFAULT 3,
  scheduled_at TIMESTAMPTZ DEFAULT NOW(),
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_jobs_queue ON jobs(queue, scheduled_at) 
WHERE attempts < max_attempts;
Enqueue:

INSERT INTO jobs (queue, payload)
VALUES ('send-email', '{"to": "user@example.com", "subject": "Hi"}');
Worker (dequeue):

WITH next_job AS (
  SELECT id FROM jobs
  WHERE queue = $1
    AND attempts < max_attempts
    AND scheduled_at <= NOW()
  ORDER BY scheduled_at
  LIMIT 1
  FOR UPDATE SKIP LOCKED
)
UPDATE jobs
SET attempts = attempts + 1
FROM next_job
WHERE jobs.id = next_job.id
RETURNING *;
The magic: FOR UPDATE SKIP LOCKED

This makes PostgreSQL a lock-free queue:

Multiple workers can pull jobs concurrently
No job is processed twice
If a worker crashes, job becomes available again
Performance:

Redis BRPOP: 0.1ms
Postgres SKIP LOCKED: 0.3ms
Negligible difference for most workloads.

PostgreSQL Feature #4: Rate Limiting
Redis (classic rate limiter):

const key = `ratelimit:${userId}`;
const count = await redis.incr(key);
if (count === 1) {
  await redis.expire(key, 60); // 60 seconds
}

if (count > 100) {
  throw new Error('Rate limit exceeded');
}
PostgreSQL:

CREATE TABLE rate_limits (
  user_id INT PRIMARY KEY,
  request_count INT DEFAULT 0,
  window_start TIMESTAMPTZ DEFAULT NOW()
);

-- Check and increment
WITH current AS (
  SELECT 
    request_count,
    CASE 
      WHEN window_start < NOW() - INTERVAL '1 minute'
      THEN 1  -- Reset counter
      ELSE request_count + 1
    END AS new_count
  FROM rate_limits
  WHERE user_id = $1
  FOR UPDATE
)
UPDATE rate_limits
SET 
  request_count = (SELECT new_count FROM current),
  window_start = CASE
    WHEN window_start < NOW() - INTERVAL '1 minute'
    THEN NOW()
    ELSE window_start
  END
WHERE user_id = $1
RETURNING request_count;
Or simpler with a window function:

CREATE TABLE api_requests (
  user_id INT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Check rate limit
SELECT COUNT(*) FROM api_requests
WHERE user_id = $1
  AND created_at > NOW() - INTERVAL '1 minute';

-- If under limit, insert
INSERT INTO api_requests (user_id) VALUES ($1);

-- Cleanup old requests periodically
DELETE FROM api_requests WHERE created_at < NOW() - INTERVAL '5 minutes';
When Postgres is better:

Need to rate limit based on complex logic (not just counts)
Want rate limit data in same transaction as business logic
When Redis is better:

Need sub-millisecond rate limiting
Extremely high throughput (millions of requests/sec)
PostgreSQL Feature #5: Sessions with JSONB
Redis:

await redis.set(`session:${sessionId}`, JSON.stringify(sessionData), 'EX', 86400);
PostgreSQL:

CREATE TABLE sessions (
  id TEXT PRIMARY KEY,
  data JSONB NOT NULL,
  expires_at TIMESTAMPTZ NOT NULL
);

CREATE INDEX idx_sessions_expires ON sessions(expires_at);

-- Insert/Update
INSERT INTO sessions (id, data, expires_at)
VALUES ($1, $2, NOW() + INTERVAL '24 hours')
ON CONFLICT (id) DO UPDATE
  SET data = EXCLUDED.data,
      expires_at = EXCLUDED.expires_at;

-- Read
SELECT data FROM sessions
WHERE id = $1 AND expires_at > NOW();
Bonus: JSONB Operators

You can query inside the session:

-- Find all sessions for a specific user
SELECT * FROM sessions
WHERE data->>'userId' = '123';

-- Find sessions with specific role
SELECT * FROM sessions
WHERE data->'user'->>'role' = 'admin';
You can't do this with Redis!

Real-World Benchmarks
I ran benchmarks on my production dataset:

Test Setup
Hardware: AWS RDS db.t3.medium (2 vCPU, 4GB RAM)
Dataset: 1 million cache entries, 10k sessions
Tool: pgbench (custom scripts)
Results
Operation	Redis	PostgreSQL	Difference
Cache SET	0.05ms	0.08ms	+60% slower
Cache GET	0.04ms	0.06ms	+50% slower
Pub/Sub	1.2ms	3.1ms	+158% slower
Queue push	0.08ms	0.15ms	+87% slower
Queue pop	0.12ms	0.31ms	+158% slower
PostgreSQL is slower... but:

All operations still under 1ms
Eliminates network hop to Redis
Reduces infrastructure complexity
Combined Operations (The Real Win)
Scenario: Insert data + invalidate cache + notify subscribers

With Redis:

await db.query('INSERT INTO posts ...');       // 2ms
await redis.del('posts:latest');                // 1ms (network hop)
await redis.publish('posts:new', data);         // 1ms (network hop)
// Total: ~4ms
With PostgreSQL:

BEGIN;
INSERT INTO posts ...;                          -- 2ms
DELETE FROM cache WHERE key = 'posts:latest';  -- 0.1ms (same connection)
NOTIFY posts_new, '...';                        -- 0.1ms (same connection)
COMMIT;
-- Total: ~2.2ms
PostgreSQL is faster when operations are combined.

When to Keep Redis
Don't replace Redis if:

1. You Need Extreme Performance
Redis: 100,000+ ops/sec (single instance)
Postgres: 10,000-50,000 ops/sec
If you're doing millions of cache reads/sec, keep Redis.

2. You're Using Redis-Specific Data Structures
Redis has:

Sorted sets (leaderboards)
HyperLogLog (unique count estimates)
Geospatial indexes
Streams (advanced pub/sub)
Postgres equivalents exist but are clunkier:

-- Leaderboard in Postgres (slower)
SELECT user_id, score
FROM leaderboard
ORDER BY score DESC
LIMIT 10;

-- vs Redis
ZREVRANGE leaderboard 0 9 WITHSCORES
3. You Have a Separate Caching Layer Requirement
If your architecture mandates a separate cache tier (e.g., microservices), keep Redis.

Migration Strategy
Don't rip out Redis overnight. Here's how I did it:

Phase 1: Side-by-Side (Week 1)
// Write to both
await redis.set(key, value);
await pg.query('INSERT INTO cache ...');

// Read from Redis (still primary)
let data = await redis.get(key);
Monitor: Compare hit rates, latency.

Phase 2: Read from Postgres (Week 2)
// Try Postgres first
let data = await pg.query('SELECT value FROM cache WHERE key = $1', [key]);

// Fallback to Redis
if (!data) {
  data = await redis.get(key);
}
Monitor: Error rates, performance.

Phase 3: Write to Postgres Only (Week 3)
// Only write to Postgres
await pg.query('INSERT INTO cache ...');
Monitor: Everything still works?

Phase 4: Remove Redis (Week 4)
# Turn off Redis
# Watch for errors
# Nothing breaks? Success!
Code Examples: Complete Implementation
Cache Module (PostgreSQL)
// cache.js
class PostgresCache {
  constructor(pool) {
    this.pool = pool;
  }

  async get(key) {
    const result = await this.pool.query(
      'SELECT value FROM cache WHERE key = $1 AND expires_at > NOW()',
      [key]
    );
    return result.rows[0]?.value;
  }

  async set(key, value, ttlSeconds = 3600) {
    await this.pool.query(
      `INSERT INTO cache (key, value, expires_at)
       VALUES ($1, $2, NOW() + INTERVAL '${ttlSeconds} seconds')
       ON CONFLICT (key) DO UPDATE
         SET value = EXCLUDED.value,
             expires_at = EXCLUDED.expires_at`,
      [key, value]
    );
  }

  async delete(key) {
    await this.pool.query('DELETE FROM cache WHERE key = $1', [key]);
  }

  async cleanup() {
    await this.pool.query('DELETE FROM cache WHERE expires_at < NOW()');
  }
}

module.exports = PostgresCache;
Pub/Sub Module
// pubsub.js
class PostgresPubSub {
  constructor(pool) {
    this.pool = pool;
    this.listeners = new Map();
  }

  async publish(channel, message) {
    const payload = JSON.stringify(message);
    await this.pool.query('SELECT pg_notify($1, $2)', [channel, payload]);
  }

  async subscribe(channel, callback) {
    const client = await this.pool.connect();

    await client.query(`LISTEN ${channel}`);

    client.on('notification', (msg) => {
      if (msg.channel === channel) {
        callback(JSON.parse(msg.payload));
      }
    });

    this.listeners.set(channel, client);
  }

  async unsubscribe(channel) {
    const client = this.listeners.get(channel);
    if (client) {
      await client.query(`UNLISTEN ${channel}`);
      client.release();
      this.listeners.delete(channel);
    }
  }
}

module.exports = PostgresPubSub;
Job Queue Module
// queue.js
class PostgresQueue {
  constructor(pool) {
    this.pool = pool;
  }

  async enqueue(queue, payload, scheduledAt = new Date()) {
    await this.pool.query(
      'INSERT INTO jobs (queue, payload, scheduled_at) VALUES ($1, $2, $3)',
      [queue, payload, scheduledAt]
    );
  }

  async dequeue(queue) {
    const result = await this.pool.query(
      `WITH next_job AS (
        SELECT id FROM jobs
        WHERE queue = $1
          AND attempts < max_attempts
          AND scheduled_at <= NOW()
        ORDER BY scheduled_at
        LIMIT 1
        FOR UPDATE SKIP LOCKED
      )
      UPDATE jobs
      SET attempts = attempts + 1
      FROM next_job
      WHERE jobs.id = next_job.id
      RETURNING jobs.*`,
      [queue]
    );

    return result.rows[0];
  }

  async complete(jobId) {
    await this.pool.query('DELETE FROM jobs WHERE id = $1', [jobId]);
  }

  async fail(jobId, error) {
    await this.pool.query(
      `UPDATE jobs
       SET attempts = max_attempts,
           payload = payload || jsonb_build_object('error', $2)
       WHERE id = $1`,
      [jobId, error.message]
    );
  }
}

module.exports = PostgresQueue;
Performance Tuning Tips
1. Use Connection Pooling
const { Pool } = require('pg');

const pool = new Pool({
  max: 20,  // Max connections
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});
2. Add Proper Indexes
CREATE INDEX CONCURRENTLY idx_cache_key ON cache(key) WHERE expires_at > NOW();
CREATE INDEX CONCURRENTLY idx_jobs_pending ON jobs(queue, scheduled_at) 
  WHERE attempts < max_attempts;
3. Tune PostgreSQL Config
# postgresql.conf
shared_buffers = 2GB           # 25% of RAM
effective_cache_size = 6GB     # 75% of RAM
work_mem = 50MB                # For complex queries
maintenance_work_mem = 512MB   # For VACUUM
4. Regular Maintenance
-- Run daily
VACUUM ANALYZE cache;
VACUUM ANALYZE jobs;

-- Or enable autovacuum (recommended)
ALTER TABLE cache SET (autovacuum_vacuum_scale_factor = 0.1);
The Results: 3 Months Later
What I saved:

âœ… $100/month (no more ElastiCache)
âœ… 50% reduction in backup complexity
âœ… One less service to monitor
âœ… Simpler deployment (one less dependency)
What I lost:

âŒ ~0.5ms latency on cache operations
âŒ Redis's exotic data structures (didn't need them)
Would I do it again? Yes, for this use case.

Would I recommend it universally? No.

Decision Matrix
Replace Redis with Postgres if:

âœ… You're using Redis for simple caching/sessions
âœ… Cache hit rate is < 95% (lots of writes)
âœ… You want transactional consistency
âœ… You're okay with 0.1-1ms slower operations
âœ… You're a small team with limited ops resources
Keep Redis if:

âŒ You need 100k+ ops/second
âŒ You use Redis data structures (sorted sets, etc.)
âŒ You have dedicated ops team
âŒ Sub-millisecond latency is critical
âŒ You're doing geo-replication
Resources
PostgreSQL Features:

LISTEN/NOTIFY Docs
SKIP LOCKED
UNLOGGED Tables
Tools:

pgBouncer - Connection pooling
pg_stat_statements - Query performance
Alternative Solutions:

Graphile Worker - Postgres-based job queue
pg-boss - Another Postgres queue
TL;DR
I replaced Redis with PostgreSQL for:

Caching â†’ UNLOGGED tables
Pub/Sub â†’ LISTEN/NOTIFY
Job queues â†’ SKIP LOCKED
Sessions â†’ JSONB tables
Results:

Saved $100/month
Reduced operational complexity
Slightly slower (0.1-1ms) but acceptable
Transactional consistency guaranteed
When to do this:

Small to medium apps
Simple caching needs
Want to reduce moving parts
When NOT to do this:

High-performance requirements (100k+ ops/sec)
Using Redis-specific features
Have dedicated ops team
Have you replaced Redis with Postgres (or vice versa)? What was your experience? Drop your benchmarks in the comments! ğŸ‘‡

P.S. - Want a follow-up on "PostgreSQL Hidden Features" or "When Redis is Actually Better"? Let me know!
